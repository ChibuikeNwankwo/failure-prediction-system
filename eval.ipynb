{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa77c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    ")\n",
    "from functions import (clean_col_names, evaluate_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a1c8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/eval_data.csv')\n",
    "\n",
    "voting1 = joblib.load(\"models/voting_stage1_model.pkl\")\n",
    "voting2 = joblib.load(\"models/voting_stage2_model.pkl\")\n",
    "thresholds_loaded = joblib.load(\"models/voting_stage1_thresholds.pkl\")\n",
    "\n",
    "# F1-optimized threshold\n",
    "t_f1 = thresholds_loaded[\"best_threshold_f1\"]\n",
    "\n",
    "# Recall-optimized threshold\n",
    "t_recall = thresholds_loaded[\"best_threshold_recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9efc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Type'] = le.fit_transform(df['Type'])\n",
    "\n",
    "feature_columns = [\n",
    "    'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]',\n",
    "    'Torque [Nm]', 'Tool wear [min]', 'Temp_diff', 'Torque_per_rpm', 'Wear_per_rpm',\n",
    "    'Type'\n",
    "]\n",
    "\n",
    "X1 = df[feature_columns]\n",
    "y1 = df['Target']\n",
    "\n",
    "X1 = clean_col_names(X1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Accuracy: 0.998\n",
      "False Negatives (missed failures): 3.030%\n",
      "False Positives (false alarms): 0.093%\n",
      "[[9634    9]\n",
      " [  10  320]]\n"
     ]
    }
   ],
   "source": [
    "# Predict failures on full dataset\n",
    "y_pred_stage1 = voting1.predict(X1)\n",
    "y_proba_stage1 = voting1.predict_proba(X1)[:, 1]\n",
    "\n",
    "cm_stage1 = confusion_matrix(y1, y_pred_stage1)\n",
    "\n",
    "# Extract useful values\n",
    "TN, FP, FN, TP = cm_stage1.ravel()\n",
    "\n",
    "# Stage 1 metrics\n",
    "acc_stage1 = accuracy_score(y1, y_pred_stage1)\n",
    "fnr_stage1 = FN / (FN + TP)   # False negative rate\n",
    "fpr_stage1 = FP / (FP + TN)   # False positive rate\n",
    "tp_rate_stage1 = TP / (TP + FN)\n",
    "\n",
    "print(f\"Stage 1 Accuracy: {acc_stage1:.3f}\")\n",
    "print(f\"False Negatives (missed failures): {fnr_stage1:.3%}\")\n",
    "print(f\"False Positives (false alarms): {fpr_stage1:.3%}\")\n",
    "print(cm_stage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff98a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9643\n",
      "           1       0.97      0.97      0.97       330\n",
      "\n",
      "    accuracy                           1.00      9973\n",
      "   macro avg       0.99      0.98      0.99      9973\n",
      "weighted avg       1.00      1.00      1.00      9973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y1, y_pred_stage1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "640cd3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Using Recall Threshold ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9643\n",
      "           1       0.98      0.97      0.97       330\n",
      "\n",
      "    accuracy                           1.00      9973\n",
      "   macro avg       0.99      0.98      0.99      9973\n",
      "weighted avg       1.00      1.00      1.00      9973\n",
      "\n",
      "Stage 1 Accuracy: 0.998\n",
      "False Negatives (missed failures): 3.333%\n",
      "False Positives (false alarms): 0.073%\n",
      "[[9636    7]\n",
      " [  11  319]]\n"
     ]
    }
   ],
   "source": [
    "# --- If you want to use the Recall-based threshold\n",
    "y_pred_recall = (y_proba_stage1 >= t_recall).astype(int)\n",
    "\n",
    "print(\"\\n=== Using Recall Threshold ===\")\n",
    "print(classification_report(y1, y_pred_recall))\n",
    "\n",
    "cm_stage1r = confusion_matrix(y1, y_pred_recall)\n",
    "\n",
    "# Extract useful values\n",
    "TN_r, FP_r, FN_r, TP_r = cm_stage1r.ravel()\n",
    "\n",
    "# Stage 1 metrics\n",
    "acc_stage1r = accuracy_score(y1, y_pred_recall)\n",
    "fnr_stage1r = FN_r / (FN_r + TP_r)   # False negative rate\n",
    "fpr_stage1r = FP_r / (FP_r + TN_r)   # False positive rate\n",
    "tp_rate_stage1r = TP_r / (TP_r + FN_r)\n",
    "\n",
    "print(f\"Stage 1 Accuracy: {acc_stage1r:.3f}\")\n",
    "print(f\"False Negatives (missed failures): {fnr_stage1r:.3%}\")\n",
    "print(f\"False Positives (false alarms): {fpr_stage1r:.3%}\")\n",
    "print(cm_stage1r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b463e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using F1 Threshold ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9643\n",
      "           1       0.98      0.97      0.97       330\n",
      "\n",
      "    accuracy                           1.00      9973\n",
      "   macro avg       0.99      0.98      0.99      9973\n",
      "weighted avg       1.00      1.00      1.00      9973\n",
      "\n",
      "Stage 1 Accuracy: 0.998\n",
      "False Negatives (missed failures): 3.333%\n",
      "False Positives (false alarms): 0.073%\n",
      "[[9636    7]\n",
      " [  11  319]]\n"
     ]
    }
   ],
   "source": [
    "# --- If you want to use the F1-based threshold\n",
    "y_pred_f1 = (y_proba_stage1 >= t_f1).astype(int)\n",
    "\n",
    "\n",
    "print(\"=== Using F1 Threshold ===\")\n",
    "print(classification_report(y1, y_pred_f1))\n",
    "\n",
    "cm_stage1f = confusion_matrix(y1, y_pred_f1)\n",
    "\n",
    "# Extract useful values\n",
    "TN_f1, FP_f1, FN_f1, TP_f1 = cm_stage1f.ravel()\n",
    "\n",
    "# Stage 1 metrics\n",
    "acc_stage1f = accuracy_score(y1, y_pred_f1)\n",
    "fnr_stage1f = FN_f1 / (FN_f1 + TP_f1)   # False negative rate\n",
    "fpr_stage1f = FP_f1 / (FP_f1 + TN_f1)   # False positive rate\n",
    "tp_rate_stage1f = TP_f1 / (TP_f1 + FN_f1)\n",
    "\n",
    "print(f\"Stage 1 Accuracy: {acc_stage1f:.3f}\")\n",
    "print(f\"False Negatives (missed failures): {fnr_stage1f:.3%}\")\n",
    "print(f\"False Positives (false alarms): {fpr_stage1f:.3%}\")\n",
    "print(cm_stage1f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missed Failures without thresholds: 10\n"
     ]
    }
   ],
   "source": [
    "# == Filter correctly predicted failures ===\n",
    "idx_correct_failures = (y1 == 1) & (y_pred_stage1 == 1)\n",
    "X_failures_correct = X1[idx_correct_failures]\n",
    "y_true_fail_type = y1[idx_correct_failures]\n",
    "df_failures_correct = df.loc[idx_correct_failures].copy()\n",
    "y2 = df_failures_correct['Failure Type']\n",
    "y2 = le.fit_transform(y2)\n",
    "\n",
    "\n",
    "diff = df[df['Target']==1].shape[0] - X_failures_correct.shape[0]\n",
    "\n",
    "print(f\"Number of Missed Failures without thresholds: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04c4f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_f1 = 0.5149\n",
      "t_recall = 0.5149\n"
     ]
    }
   ],
   "source": [
    "print(f\"t_f1 = {t_f1:.4f}\")\n",
    "print(f\"t_recall = {t_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2a9df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Classifier Metrics:\n",
      "Accuracy: 0.98125\n",
      "Precision: 0.9811940208321788\n",
      "Recall: 0.98125\n",
      "F1: 0.9811647516414503\n",
      "MCC: 0.9740089400817524\n",
      "ROC_AUC: 0.9979183199834033\n",
      "Confusion_Matrix: [[112   0   0   0]\n",
      " [  1  75   1   1]\n",
      " [  1   0  91   0]\n",
      " [  0   2   0  36]]\n",
      "AUC_PR: 0.9904033000803643\n",
      "\n",
      "Stage 2 (Failure Cause) Accuracy: 0.981\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       112\n",
      "           1       0.97      0.96      0.97        78\n",
      "           2       0.99      0.99      0.99        92\n",
      "           3       0.97      0.95      0.96        38\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.98      0.97      0.98       320\n",
      "weighted avg       0.98      0.98      0.98       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 predictions (for correctly predicted failures)\n",
    "y_pred_stage2 = voting2.predict(X_failures_correct)\n",
    "y_proba_stage2 = voting2.predict_proba(X_failures_correct)\n",
    "metrics_voting2 = evaluate_metrics(y2, y_pred_stage2, y_proba_stage2)\n",
    "\n",
    "print(\"\\nVoting Classifier Metrics:\")\n",
    "for k, v in metrics_voting2.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "acc_stage2 = accuracy_score(y2, y_pred_stage2)\n",
    "report_stage2 = classification_report(y2, y_pred_stage2)\n",
    "\n",
    "print(f\"\\nStage 2 (Failure Cause) Accuracy: {acc_stage2:.3f}\")\n",
    "print()\n",
    "print(report_stage2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfd4eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System Summary:\n",
      "---------------\n",
      "Total records: 9973\n",
      "Actual failures: 330\n",
      "Predicted failures: 329\n",
      "\n",
      "Stage 1 (Failure Detection):\n",
      "- Accuracy: 0.998\n",
      "- Correct detections: 320 (96.97%)\n",
      "- Missed failures (false negatives): 10 (3.03%)\n",
      "- False alarms (false positives): 9 (0.09%)\n",
      "\n",
      "Stage 1 (Failure Detection Recall Focused):\n",
      "- Accuracy: 0.998\n",
      "- Correct detections: 319 (96.67%)\n",
      "- Missed failures (false negatives): 11 (3.33%)\n",
      "- False alarms (false positives): 7 (0.07%)\n",
      "\n",
      "Stage 1 (Failure Detection F1 Focused):\n",
      "- Accuracy: 0.998\n",
      "- Correct detections: 319 (96.67%)\n",
      "- Missed failures (false negatives): 11 (3.33%)\n",
      "- False alarms (false positives): 7 (0.07%)\n",
      "\n",
      "Stage 2 (Failure Cause Diagnosis):\n",
      "- Accuracy: 0.981\n",
      "- Correct cause predictions among detected failures: 98.12%\n",
      "\n",
      "End-to-End:\n",
      "- Overall system correctly predicted 95.15% of all failure causes end-to-end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total samples\n",
    "total_samples = len(y1)\n",
    "failures_actual = np.sum(y1 == 1)\n",
    "failures_predicted = np.sum(y_pred_stage1 == 1)\n",
    "\n",
    "# Stage 1 effectiveness\n",
    "correct_detections = TP\n",
    "missed_failures = FN\n",
    "false_alarms = FP\n",
    "\n",
    "# Stage 1 effectiveness recall focused\n",
    "correct_detections_r = TP_r\n",
    "missed_failures_r = FN_r\n",
    "false_alarms_r = FP_r\n",
    "\n",
    "# Stage 1 effectiveness f1 focused\n",
    "correct_detections_f1 = TP_f1\n",
    "missed_failures_f1 = FN_f1\n",
    "false_alarms_f1 = FP_f1\n",
    "\n",
    "# Stage 2 effectiveness (among correctly detected failures)\n",
    "correct_cause_preds = acc_stage2 * correct_detections\n",
    "\n",
    "print(f\"\"\"\n",
    "System Summary:\n",
    "---------------\n",
    "Total records: {total_samples}\n",
    "Actual failures: {failures_actual}\n",
    "Predicted failures: {failures_predicted}\n",
    "\n",
    "Stage 1 (Failure Detection):\n",
    "- Accuracy: {acc_stage1:.3f}\n",
    "- Correct detections: {correct_detections} ({tp_rate_stage1:.2%})\n",
    "- Missed failures (false negatives): {missed_failures} ({fnr_stage1:.2%})\n",
    "- False alarms (false positives): {false_alarms} ({fpr_stage1:.2%})\n",
    "\n",
    "Stage 1 (Failure Detection Recall Focused):\n",
    "- Accuracy: {acc_stage1r:.3f}\n",
    "- Correct detections: {correct_detections_r} ({tp_rate_stage1r:.2%})\n",
    "- Missed failures (false negatives): {missed_failures_r} ({fnr_stage1r:.2%})\n",
    "- False alarms (false positives): {false_alarms_r} ({fpr_stage1r:.2%})\n",
    "\n",
    "Stage 1 (Failure Detection F1 Focused):\n",
    "- Accuracy: {acc_stage1f:.3f}\n",
    "- Correct detections: {correct_detections_f1} ({tp_rate_stage1f:.2%})\n",
    "- Missed failures (false negatives): {missed_failures_f1} ({fnr_stage1f:.2%})\n",
    "- False alarms (false positives): {false_alarms_f1} ({fpr_stage1f:.2%})\n",
    "\n",
    "Stage 2 (Failure Cause Diagnosis):\n",
    "- Accuracy: {acc_stage2:.3f}\n",
    "- Correct cause predictions among detected failures: {acc_stage2*100:.2f}%\n",
    "\n",
    "End-to-End:\n",
    "- Overall system correctly predicted {acc_stage2 * tp_rate_stage1 * 100:.2f}% of all failure causes end-to-end.\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
